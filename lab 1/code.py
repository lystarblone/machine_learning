import random  # Импортируем модуль для генерации случайных чисел
import matplotlib.pyplot as plt  # Импортируем библиотеку для построения графиков

weights = []  # Список для хранения значений весов после каждой итерации обучения

def show_learning(w):
  """
  Функция для вывода текущих значений весов на экран
  """
  weights.append(w)  # Добавляем текущие значения весов в список для отслеживания изменений
  # Печатаем значения весов в удобном формате (с точностью до 2 знаков после запятой)
  print('w0 =', '%5.2f' % w[0], ', w1 =', '%5.2f' % w[1], ', w2 =', '%5.2f' % w[2])

# Инициализация случайного генератора и параметров обучения
random.seed(7)  # Устанавливаем начальное значение генератора случайных чисел для воспроизводимости результатов
LEARNING_RATE = 0.1  # Задаём скорость обучения, которая контролирует величину корректировок весов на каждой итерации

# Обучающие данные (входные данные) - список примеров для тренировки сети
x_train = [
   (1.0, -1.7, 1.9), (1.0, -0.8, 1.9), (1.0, 0.7, 1.9), (1.0, 0.8, 1.9), (1.0, 1.7, 1.9),
   (1.0, -1.7, 0.5), (1.0, -0.8, 0.5), (1.0, 0.7, 0.5), (1.0, 0.8, 0.5), (1.0, 1.7, 0.5),
   (1.0, -1.7, -0.6), (1.0, -0.8, -0.6), (1.0, 0.7, -0.6), (1.0, 0.8, -0.6), (1.0, 1.7, -0.6),
   (1.0, -1.7, -0.8), (1.0, -0.8, -0.8), (1.0, 0.7, -0.8), (1.0, 0.8, -0.8), (1.0, 1.7, -0.8),
   (1.0, -1.7, -1.9), (1.0, -0.8, -1.9), (1.0, 0.7, -1.9), (1.0, 0.8, -1.9), (1.0, 1.7, -1.9)
]  # Входные данные (x1, x2 и дополнительный компонент, равный 1)

# Истинные выходные значения для каждого обучающего примера (y_train)
y_train = [
   1.0, 1.0, 1.0, -1.0, -1.0,
   1.0, 1.0, -1.0, -1.0, -1.0,
   1.0, -1.0, -1.0, -1.0, -1.0,
   -1.0, -1.0, -1.0, -1.0, -1.0,
   -1.0, -1.0, -1.0, -1.0, -1.0
]  # Истинные значения для каждого входного примера

# Список индексов для случайного порядка обучения
index_list = [i for i in range(len(x_train))]  # Список чисел от 0 до длины x_train, для случайного перемешивания

# Инициализация начальных весов случайными значениями
w = [0.2, -0.6, 0.25]  # Начальные веса: w0, w1, w2
show_learning(w)  # Печатаем начальные значения весов

# Функция для вычисления выхода нейрона (персептрона) для данного входа
def compute_output(w, x):
  z = 0.0  # Инициализируем переменную для вычисления взвешенной суммы
  for i in range(len(w)):  # Проходим по всем весам
      z += x[i] * w[i]  # Вычисляем взвешенную сумму входов
  if z < 0:  # Применяем пороговую функцию активации
      return -1  # Если сумма меньше 0, то выход -1
  else:
      return 1  # Если сумма больше или равна 0, то выход 1

# Цикл обучения персептрона
all_correct = False  # Флаг, который будет проверять, правильно ли классифицированы все примеры
while not all_correct:
  all_correct = True  # Сначала предполагаем, что все правильно
  random.shuffle(index_list)  # Перемешиваем порядок примеров, чтобы обучение не было детерминированным
  for i in index_list:
      x = x_train[i]  # Получаем текущий обучающий пример
      y = y_train[i]  # Получаем истинный выход для текущего примера
      p_out = compute_output(w, x)  # Вычисляем предсказанный выход нейрона

      if y != p_out:  # Если предсказание неверное, обновляем веса
          for j in range(0, len(w)):  # Пробегаем по всем весам
              w[j] += (y * LEARNING_RATE * x[j])  # Обновляем вес по правилу: w = w + (learning_rate * x * y)
          all_correct = False  # Устанавливаем флаг, что не все правильно
          show_learning(w)  # Выводим обновленные веса на каждом шаге

# График 1: Отображаем обучающие данные
fig, ax = plt.subplots()  # Создаем новую фигуру для графика
plt.xlim([-2.0, 2.0])  # Устанавливаем пределы по оси X

# Отображаем данные в зависимости от их метки
for i in range(len(x_train)):
  if y_train[i] == -1:
      ax.plot(x_train[i][1], x_train[i][2], 'r_')  # Для метки -1 рисуем красные подчеркивания
  else:
      ax.plot(x_train[i][1], x_train[i][2], 'r+')  # Для метки 1 рисуем красные крестики

plt.xlabel("x1")  # Подпись оси X
plt.ylabel("x2")  # Подпись оси Y

# Строим разделяющую гиперплоскость для каждого шага обучения
x1 = [x_train[0][1], x_train[-1][1]]  # Для построения линии разделения
for i in weights:
  x2 = (-(i[1]/i[2]) * x1[0] - (i[0]/i[2]), -(i[1]/i[2]) * x1[1] - (i[0]/i[2]))  # Вычисляем соответствующие значения y
  ax.plot(x1, x2)  # Строим линию разделения на графике

# График 2: Тот же график, но только для последних весов
fig, ax = plt.subplots()  # Создаем новый график
plt.xlim([-2.0, 2.0])  # Устанавливаем пределы оси X

# Повторно отображаем обучающие данные
for i in range(len(x_train)):
  if y_train[i] == -1:
      ax.plot(x_train[i][1], x_train[i][2], 'r_')  # Отображаем красные подчеркивания для класса -1
  else:
      ax.plot(x_train[i][1], x_train[i][2], 'r+')  # Отображаем красные крестики для класса 1

plt.xlabel("x1")  # Подпись оси X
plt.ylabel("x2")  # Подпись оси Y

# Построение линии разделения на основе последних обученных весов
x1_n = [x_train[0][1], x_train[-1][1]]  # Для построения линии разделения
for j in x1_n:
  # Вычисляем соответствующие y для последнего шага
  x2_n = (-(weights[-1][1]/weights[-1][2])* x1[0] - (weights[-1][0]/weights[-1][2]), -(weights[-1][1]/weights[-1][2])*x1[1] - (weights[-1][0]/weights[-1][2]))
  ax.plot(x1_n, x2_n)  # Строим линию разделения

plt.show()  # Показываем графики