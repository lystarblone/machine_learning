{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Практическое занятие 3 Обучение двухслойной ИНС реализации функции исключающее ИЛИ (XOR) с помощью алгоритма обратного распространения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом примере кода реализован алгоритм обратного распространения для двухуровневой сети и показано, как использовать его для обучения сети функции исключающее ИЛИ (XOR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код инициализации в приведенном ниже фрагменте кода аналогичен тому, что был рассмотрен в примере с обучением персептрона (практическое задание 1). Следует отметить, что мы начали использовать массивы NumPy, чтобы использовать некоторые функции NumPy. То же самое верно и для нашего генератора случайных чисел - мы вызываем np.random.seed вместо просто random.seed. Ответьте, зачем?\n",
    "\n",
    "Здесь, в обучающих примерах мы изменили истинное значение на значение от 0,0 до 1,0, поскольку для выходного нейрона мы используем теперь логистическую сигмоидальную функцию в качестве функции активации, а её выходной диапазон не доходит до -1,0. Ответьте, почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(3) \n",
    "LEARNING_RATE = 0.1\n",
    "index_list = [0, 1, 2, 3]\n",
    "\n",
    "x_train = [np.array([1.0, -1.0, -1.0]),\n",
    "           np.array([1.0, -1.0, 1.0]),\n",
    "           np.array([1.0, 1.0, -1.0]),\n",
    "           np.array([1.0, 1.0, 1.0])]\n",
    "y_train = [0.0, 1.0, 1.0, 0.0] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберите приведённый код, объясните каждую строчку.\n",
    "\n",
    "В следующем фрагменте кода мы объявляем переменные для хранения состояния трёх нейронов. Реальная реализация обычно параметризуется, чтобы иметь возможность выбирать количество входных данных, слоев и количество нейронов в каждом слое, но в этом примере все эти параметры жестко закодированы, чтобы код был удобен для прочтения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_w(input_count):\n",
    "    weights = np.zeros(input_count+1)\n",
    "    for i in range(1, (input_count+1)):\n",
    "        weights[i] = np.random.uniform(-1.0, 1.0)\n",
    "    return weights\n",
    "\n",
    "n_w = [neuron_w(2), neuron_w(2), neuron_w(2)]\n",
    "n_y = [0, 0, 0]\n",
    "n_error = [0, 0, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберите приведённый код, объясните каждую строчку.\n",
    "\n",
    "Следующий фрагмент кода начинается с функции печати всех девяти весов сети (каждый оператор print печатает трехэлементный вектор весов). Функция forward_pass сначала вычисляет выходные данные нейронов 0 и 1 с одинаковыми входными данными (входными данными из обучающего примера), а затем помещает их выходные данные в массив вместе со значением смещения 1,0 для использования в качестве входных данных для нейрона 2. То есть эта функция определяет топологию сети. Мы используем tanh для нейронов первого слоя и логистическую сигмовидную функцию для выходного нейрона.\n",
    "\n",
    "Функция reverse_pass начинается с вычисления производной функции ошибок, а затем вычисляет производную функции активации для выходного нейрона. Погрешность выходного нейрона вычисляется путем их умножения. Затем мы продолжаем распространять ошибку на каждый из двух нейронов скрытого слоя. Это делается путем вычисления производных их функций активации и умножения этих производных на член ошибки выходного нейрона и на вес выходного нейрона.\n",
    "\n",
    "Наконец, функция Adjust_Weights корректирует веса для каждого из трех нейронов. Поправочный коэффициент вычисляется путем умножения входных данных на скорость обучения и погрешность для рассматриваемого нейрона."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_learning():\n",
    "    print('Current weights:')\n",
    "    for i, w in enumerate(n_w):\n",
    "        print('neuron ', i, ': w0 =', '%5.2f' % w[0],\n",
    "              ', w1 =', '%5.2f' % w[1], ', w2 =',\n",
    "              '%5.2f' % w[2])\n",
    "    print('----------------')\n",
    "\n",
    "def forward_pass(x):\n",
    "    global n_y\n",
    "    n_y[0] = np.tanh(np.dot(n_w[0], x)) \n",
    "    n_y[1] = np.tanh(np.dot(n_w[1], x)) \n",
    "    n2_inputs = np.array([1.0, n_y[0], n_y[1]]) \n",
    "    z2 = np.dot(n_w[2], n2_inputs)\n",
    "    n_y[2] = 1.0 / (1.0 + np.exp(-z2))\n",
    "\n",
    "def backward_pass(y_truth):\n",
    "    global n_error\n",
    "    error_prime = -(y_truth - n_y[2]) \n",
    "    derivative = n_y[2] * (1.0 - n_y[2]) \n",
    "    n_error[2] = error_prime * derivative\n",
    "    derivative = 1.0 - n_y[0]**2\n",
    "    n_error[0] = n_w[2][1] * n_error[2] * derivative\n",
    "    derivative = 1.0 - n_y[1]**2 \n",
    "    n_error[1] = n_w[2][2] * n_error[2] * derivative\n",
    "\n",
    "def adjust_weights(x):\n",
    "    global n_w\n",
    "    n_w[0] -= (x * LEARNING_RATE * n_error[0])\n",
    "    n_w[1] -= (x * LEARNING_RATE * n_error[1])\n",
    "    n2_inputs = np.array([1.0, n_y[0], n_y[1]])\n",
    "    n_w[2] -= (n2_inputs * LEARNING_RATE * n_error[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберите приведённый выше код, объясните каждую строчку. Составьте алгоритм по этому коду.\n",
    "\n",
    "Осталось реализовать последнюю часть кода — это цикл обучения, показанный во фрагменте кода ниже, который напомниает цикл обучения для примера с персептроном из первого практического задания.\n",
    "Мы выбираем обучающие примеры в случайном порядке, вызываем функции forward_pass, back_pass и Adjust_weights, а затем печатаем значения весов с помощью функции show_learning.\n",
    "\n",
    "Разберите приведённый ниже код, объясните каждую строчку. Составьте алгоритм по этому коду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current weights:\n",
      "neuron  0 : w0 =  0.69 , w1 =  0.78 , w2 =  0.76\n",
      "neuron  1 : w0 =  0.40 , w1 = -0.58 , w2 = -0.56\n",
      "neuron  2 : w0 = -0.44 , w1 =  1.02 , w2 =  0.88\n",
      "----------------\n",
      "Current weights:\n",
      "neuron  0 : w0 =  0.70 , w1 =  0.77 , w2 =  0.77\n",
      "neuron  1 : w0 =  0.41 , w1 = -0.59 , w2 = -0.55\n",
      "neuron  2 : w0 = -0.43 , w1 =  1.02 , w2 =  0.89\n",
      "----------------\n",
      "Current weights:\n",
      "neuron  0 : w0 =  0.70 , w1 =  0.77 , w2 =  0.77\n",
      "neuron  1 : w0 =  0.40 , w1 = -0.59 , w2 = -0.56\n",
      "neuron  2 : w0 = -0.45 , w1 =  1.01 , w2 =  0.89\n",
      "----------------\n",
      "Current weights:\n",
      "neuron  0 : w0 =  0.71 , w1 =  0.77 , w2 =  0.76\n",
      "neuron  1 : w0 =  0.41 , w1 = -0.59 , w2 = -0.57\n",
      "neuron  2 : w0 = -0.44 , w1 =  1.01 , w2 =  0.90\n",
      "----------------\n",
      "x1 = -1.0 , x2 = -1.0 , y = 0.4244\n",
      "x1 = -1.0 , x2 =  1.0 , y = 0.6306\n",
      "x1 =  1.0 , x2 = -1.0 , y = 0.6277\n",
      "x1 =  1.0 , x2 =  1.0 , y = 0.4976\n"
     ]
    }
   ],
   "source": [
    "# Network training loop.\n",
    "all_correct = False\n",
    "while not all_correct: # Train until converged\n",
    "    all_correct = True\n",
    "    np.random.shuffle(index_list) # Randomize order\n",
    "    for i in index_list: # Train on all examples\n",
    "        forward_pass(x_train[i])\n",
    "        backward_pass(y_train[i])\n",
    "        adjust_weights(x_train[i])\n",
    "        show_learning() # Show updated weights\n",
    "    for i in range(len(x_train)): # Check if converged\n",
    "        forward_pass(x_train[i])\n",
    "        print('x1 =', '%4.1f' % x_train[i][1], ', x2 =',\n",
    "              '%4.1f' % x_train[i][2], ', y =',\n",
    "              '%.4f' % n_y[2])\n",
    "        if(((y_train[i] < 0.5) and (n_y[2] >= 0.5))\n",
    "                or ((y_train[i] >= 0.5) and (n_y[2] < 0.5))):\n",
    "            all_correct = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
