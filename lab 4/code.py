import matplotlib.pyplot as plt  # Импорт библиотеки для построения графиков
import numpy as np  # Импорт библиотеки для работы с массивами и математическими операциями

np.random.seed(6)  # Устанавливаем начальное значение для генератора случайных чисел для воспроизводимости

# Функция для создания случайных весов для нейронов
def neuron_w(input_count):
   weights = np.zeros(input_count + 1)  # Создаем массив весов, включая смещение (bias)
   # Заполняем веса случайными значениями в диапазоне от -1 до 1
   for i in range(1, (input_count + 1)):  # Проходим по всем весам, начиная с первого (смещение оставляем 0)
       weights[i] = np.random.uniform(-1.0, 1.0)  # Генерируем случайное число в диапазоне [-1, 1] для веса
   return weights  # Возвращаем массив весов

# Генерируем 9 нейронов с 4 входами и случайными весами для каждого из них
n_w = [neuron_w(4) for _ in range(9)]  # Для каждого нейрона генерируем веса, всего 9 нейронов

# Генерация входных данных для обучения (бинарные векторы)
x0 = [1 for _ in range(16)]  # Массив смещений, равный 1 (16 значений)
x1 = [1 if i // 8 == 1 else 0 for i in range(16)]  # Вектор: 1, если i >= 8, иначе 0
x2 = [1 if i // 4 % 2 != 0 else 0 for i in range(16)]  # Вектор: меняется каждые 4 элемента
x3 = [1 if i // 2 % 2 != 0 else 0 for i in range(16)]  # Вектор: меняется каждые 2 элемента
x4 = [1 if i % 2 != 0 else 0 for i in range(16)]  # Вектор: меняется на каждом элементе

# Формируем обучающий набор данных, комбинируя все входные вектора
x_train = np.array(list(zip(x0, x1, x2, x3, x4)))  # Создаем обучающий набор данных (16 строк по 5 признаков)

# Заданные целевые значения для обучения нейронной сети (классы: 1 или -1)
y_train = np.array([1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, -1, -1])  # Целевые метки для каждого примера

# Класс для нейронной сети
class Network:
   LEARNING_RATE = 0.1  # Устанавливаем скорость обучения (параметр, регулирующий шаг обновления весов)

   def __init__(self, w: list, x_train: list, y_train: list) -> None:
       """
       Инициализация нейронной сети:
       - w: веса сети
       - x_train: входные данные
       - y_train: целевые значения
       """
       self.w = w  # Инициализация весов
       self.x_train = x_train  # Инициализация входных данных
       self.y_train = y_train  # Инициализация целевых значений
       self.n_y = [0 for _ in range(len(w))]  # Список для хранения выходных значений нейронов
       self.n_error = [0 for _ in range(len(w))]  # Список для хранения ошибок нейронов
       self.index_list = [i for i in range(len(self.x_train))]  # Индексы всех примеров для случайной выборки
       self.epoch = 0  # Счетчик количества эпох (итераций)
       self.weighted_sum = 0  # Переменная для вычисления взвешенной суммы входов
       self.MSE = []  # Массив для хранения значений ошибки на каждой эпохе (среднеквадратичная ошибка)

   def show_learning(self):
       """
       Функция для вывода текущих значений весов на каждой эпохе.
       """
       print(f'epoch - {self.epoch}')  # Выводим номер текущей эпохи
       for i, w in enumerate(n_w):  # Для каждого нейрона выводим значения его весов
           print(f'neuron {i}: w0 = {w[0]}, w1 = {w[1]}, w2 = {w[2]}, w3 = {w[3]}, w4 = {w[4]}')  # Выводим веса каждого нейрона
           print('----------------')  # Разделитель между нейронами

   def forward_pass(self, x):
       """
       Прямой проход через нейронную сеть:
       1. Вычисляем выходы нейронов на каждом слое.
       2. Используем функцию активации tanh для преобразования линейной комбинации входов в выход.
       """
       for i in range(4):  # Проходим по входному слою (4 нейрона)
           self.n_y[i] = np.tanh(np.dot(self.w[i], x))  # Для каждого нейрона вычисляем выход, используя функцию активации tanh
       self.layer2_input = np.array([1.0] + [self.n_y[i] for i in range(4)])  # Добавляем смещение для второго слоя

       for i in range(4):  # Проходим по второму слою нейронов (4 нейрона)
           self.n_y[i + 4] = np.tanh(np.dot(self.w[i + 4], self.layer2_input))  # Рассчитываем выход для каждого нейрона второго слоя

       # Для третьего слоя (выходного слоя)
       self.output_layer_input = np.array([1.0] + [self.n_y[i + 4] for i in range(4)])  # Добавляем смещение для выходного слоя
       self.n_y[8] = np.tanh(np.dot(self.w[8], self.output_layer_input))  # Рассчитываем выход для последнего нейрона

   def backward_pass(self, y):
       """
       Обратный проход:
       1. Рассчитываем ошибку для выходного нейрона.
       2. Обновляем ошибки для каждого слоя в сети, используя производную от активационной функции tanh.
       """
       self.error_prime = -(y - self.n_y[8])  # Рассчитываем ошибку для выходного нейрона
       self.derivative = 1.0 - self.n_y[8] ** 2  # Производная от функции активации tanh
       self.n_error[8] = self.error_prime * self.derivative  # Ошибка для выходного нейрона

       for i in range(4):  # Процесс для второго слоя нейронов
           self.derivative = 1.0 - self.n_y[i + 4] ** 2  # Производная от tanh для каждого нейрона второго слоя
           self.n_error[i + 4] = self.w[8][i + 1] * self.n_error[8] * self.derivative  # Ошибка для нейронов второго слоя

       for i in range(4):  # Процесс для первого слоя нейронов
           self.weight_sum = 0  # Сумма ошибок для каждого нейрона первого слоя
           for j in range(4):  # Суммируем ошибки для всех нейронов второго слоя
               self.weight_sum += self.n_error[j + 4] * self.w[i][j + 1]
           self.derivative = 1.0 - self.n_y[i] ** 2  # Производная от tanh для нейронов первого слоя
           self.n_error[i] = self.weight_sum * self.derivative  # Ошибка для нейронов первого слоя

   def adjust_weights(self, x):
       """
       Корректировка весов на основе ошибки:
       1. Обновляем веса каждого слоя нейронной сети с помощью градиентного спуска.
       2. Используем скорость обучения (LEARNING_RATE).
       """
       for i in range(4):  # Обновляем веса для первого слоя
           self.w[i] -= (x * Network.LEARNING_RATE * self.n_error[i])  # Обновление веса для каждого нейрона первого слоя
           self.w[i + 4] -= (self.layer2_input * Network.LEARNING_RATE * self.n_error[i + 4])  # Обновление весов для второго слоя
       self.w[8] -= (self.output_layer_input * Network.LEARNING_RATE * self.n_error[8])  # Обновление веса для выходного слоя

   def learning(self):
       """
       Основной цикл обучения:
       1. Выполняем прямой и обратный проход на каждом шаге.
       2. Обновляем веса и проверяем, достигнута ли необходимая точность.
       """
       all_correct = False  # Флаг, показывающий, что все ответы сети правильные
       while not all_correct:  # Повторяем обучение до тех пор, пока все ответы не будут правильными
           self.H = 0  # Сумма ошибок на текущей эпохе
           all_correct = True  # Изначально считаем, что все ответы правильные
           self.epoch += 1  # Увеличиваем счетчик эпох
           np.random.shuffle(self.index_list)  # Случайным образом перемешиваем индексы данных для обучения
           for i in self.index_list:  # Проходим по всем примерам в обучающем наборе
               self.forward_pass(self.x_train[i])  # Прямой проход
               self.backward_pass(self.y_train[i])  # Обратный проход
               self.adjust_weights(self.x_train[i])  # Обновление весов
               self.H += (self.y_train[i] - self.n_y[8]) ** 2  # Добавляем квадрат ошибки для текущего примера
           self.H /= len(self.x_train)  # Среднеквадратичная ошибка (MSE) на текущей эпохе
           self.MSE.append(self.H)  # Сохраняем MSE для анализа

           # Проверка, все ли ответы правильные
           for i in range(len(self.x_train)):
               self.forward_pass(self.x_train[i])  # Прямой проход для примера
               output = 1 if self.n_y[8] >= 0 else -1  # Преобразуем выход в бинарное значение (1 или -1)
               if self.y_train[i] != output:  # Если выход не совпадает с целевым значением, то ошибка
                   all_correct = False  # Если хотя бы один ответ неправильный, продолжаем обучение

       self.show_learning()  # Выводим текущие веса после обучения

   def show_MSE(self):
       """
       Функция для отображения графика MSE (среднеквадратичной ошибки).
       """
       plt.title('MSE')  # Заголовок графика
       plt.xlabel('epoch')  # Подпись оси X (номер эпохи)
       plt.ylabel('H')  # Подпись оси Y (значение MSE)
       plt.plot([i for i in range(len(self.MSE))], self.MSE)  # Построение графика
       plt.show()  # Отображение графика

# Запуск обучения нейронной сети
if __name__ == '__main__':
   nn = Network(n_w, x_train, y_train)  # Создаем объект сети
   nn.learning()  # Запускаем процесс обучения
   nn.show_MSE()  # Показываем график ошибки на протяжении обучения